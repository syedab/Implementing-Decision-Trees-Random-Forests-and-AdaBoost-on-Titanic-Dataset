{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "af6bb647-50c9-cb5e-5b00-ea49474ff254"
   },
   "source": [
    "## Assignment 5\n",
    "Topics: Decision Trees, Random Forests, and Boosting ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "d66580d1-f255-86b9-e5c0-f012342ab8d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "train shape: (1047, 11) test shape: (262, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Hart, Mr. Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>394140</td>\n",
       "      <td>6.8583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Johnston, Master. William Arthur \"Willie\"</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Goodwin, Master. William Frederick</td>\n",
       "      <td>male</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>CA 2144</td>\n",
       "      <td>46.9000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass  Survived                                       Name   Sex   Age  \\\n",
       "0       3         0                            Hart, Mr. Henry  male   NaN   \n",
       "1       3         0  Johnston, Master. William Arthur \"Willie\"  male   NaN   \n",
       "2       3         0         Goodwin, Master. William Frederick  male  11.0   \n",
       "\n",
       "   SibSp  Parch      Ticket     Fare Cabin Embarked  \n",
       "0      0      0      394140   6.8583   NaN        Q  \n",
       "1      1      2  W./C. 6607  23.4500   NaN        S  \n",
       "2      5      2     CA 2144  46.9000   NaN        S  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some basic imports\n",
    "from scipy import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from functools import partial\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#loading training dataset\n",
    "train = pd.read_csv(r'C:\\Users\\syeda\\Desktop\\Machine Learning\\Assignment-5\\train.csv')\n",
    "#loading test dataset\n",
    "test = pd.read_csv(r'C:\\Users\\syeda\\Desktop\\Machine Learning\\Assignment-5\\test.csv')\n",
    "\n",
    "#displaying the shapes of the train and test data\n",
    "print(\"train shape: {} test shape: {}\".format(train.shape, test.shape))\n",
    "#top 3 elements of train\n",
    "train.head(3)\n",
    "\n",
    "#top 3 elements of test\n",
    "test.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referencing from https://www.kaggle.com/competitions/titanic/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "72063fe1-8b92-850b-3ede-5e030c7429ae"
   },
   "outputs": [],
   "source": [
    "\n",
    "original_train = train.copy() # Using 'copy()' allows to clone the dataset, creating a different object with the same values\n",
    "\n",
    "# Feature engineering steps taken from Sina and Anisotropic, with minor changes to avoid warnings\n",
    "full_data = [train, test]\n",
    "\n",
    "# Feature that tells whether a passenger had a cabin on the Titanic\n",
    "train['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n",
    "test['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n",
    "\n",
    "# Create new feature FamilySize as a combination of SibSp and Parch\n",
    "for dataset in full_data:\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "# Create new feature IsAlone from FamilySize\n",
    "for dataset in full_data:\n",
    "    dataset['IsAlone'] = 0\n",
    "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "# Remove all NULLS in the Embarked column\n",
    "for dataset in full_data:\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n",
    "# Remove all NULLS in the Fare column\n",
    "for dataset in full_data:\n",
    "    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\n",
    "\n",
    "# Remove all NULLS in the Age column\n",
    "for dataset in full_data:\n",
    "    age_avg = dataset['Age'].mean()\n",
    "    age_std = dataset['Age'].std()\n",
    "    age_null_count = dataset['Age'].isnull().sum()\n",
    "    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n",
    "    # Next line has been improved to avoid warning\n",
    "    dataset.loc[np.isnan(dataset['Age']), 'Age'] = age_null_random_list\n",
    "    dataset['Age'] = dataset['Age'].astype(int)\n",
    "\n",
    "# Define function to extract titles from passenger names\n",
    "def get_title(name):\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Name'].apply(get_title)\n",
    "# Group all non-common titles into one single grouping \"Rare\"\n",
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
    "\n",
    "for dataset in full_data:\n",
    "    # Mapping Sex\n",
    "    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "    \n",
    "    # Mapping titles\n",
    "    title_mapping = {\"Mr\": 1, \"Master\": 2, \"Mrs\": 3, \"Miss\": 4, \"Rare\": 5}\n",
    "    dataset['Title'] = dataset['Title'].map(title_mapping)\n",
    "    dataset['Title'] = dataset['Title'].fillna(0)\n",
    "\n",
    "    # Mapping Embarked\n",
    "    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n",
    "    \n",
    "    # Mapping Fare\n",
    "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n",
    "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
    "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n",
    "    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n",
    "    dataset['Fare'] = dataset['Fare'].astype(int)\n",
    "    \n",
    "    # Mapping Age\n",
    "    dataset.loc[ dataset['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\n",
    "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n",
    "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n",
    "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n",
    "    dataset.loc[ dataset['Age'] > 64, 'Age'] ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "0486bde8-a09f-93c6-832d-c5c9760e2f6e"
   },
   "outputs": [],
   "source": [
    "drop_elements = ['Name', 'Ticket', 'Cabin', 'SibSp']\n",
    "train = train.drop(drop_elements, axis = 1)\n",
    "test  = test.drop(drop_elements, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "abef639b-2b31-4209-20ef-d8ba15c4de74"
   },
   "source": [
    "## Visualization of  the  processed data from titanic dataset ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "b587a7d3-36be-408e-0dee-6fd292e9621b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Has_Cabin</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>IsAlone</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass  Survived  Sex  Age  Parch  Fare  Embarked  Has_Cabin  FamilySize  \\\n",
       "0       1         1    0    1      0     3         1          1           1   \n",
       "1       3         0    1    2      0     0         0          0           1   \n",
       "2       3         0    1    2      0     1         0          0           3   \n",
       "\n",
       "   IsAlone  Title  \n",
       "0        1      4  \n",
       "1        1      1  \n",
       "2        0      1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "ce843f51-6f28-ef63-5f21-5f6bf8802e7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (1047, 10), test: (262, 10)\n",
      "train: (1047,), test: (262,)\n",
      "10\n",
      "1047\n"
     ]
    }
   ],
   "source": [
    "val = \"Survived\"\n",
    "#var variable is selected to be dropped from the table\n",
    "X = train.drop([val], axis=1)\n",
    "X_test = test.drop([val], axis=1)\n",
    "y = train[val]\n",
    "y_test = test[val]\n",
    "print(\"train: {}, test: {}\".format(X.shape, X_test.shape))\n",
    "print(\"train: {}, test: {}\".format(y.shape, y_test.shape))\n",
    "print((np.array(X)).shape[1])\n",
    "print((np.array(y)).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "05abd10a-b698-dc8a-f745-59436608e38c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individaul Survived: 0.387775, Individual Not Survivied: 0.612225\n"
     ]
    }
   ],
   "source": [
    "def accuracy(val_1, val_2):\n",
    "    #accuracy is calculated as a sum when two values are equal and its divided my the length of the shape\n",
    "    acc = np.sum(val_1 == val_2) / val_1.shape[0]\n",
    "    return acc\n",
    "print(\"Individaul Survived: {:.6f}, Individual Not Survivied: {:.6f}\".format(y.sum()/len(y), 1-y.sum()/len(y)))\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Random Forest \n",
    "Also adding input parameters:\n",
    "criterion - With Gini impurity, or entropy.,\n",
    "max_depth, \n",
    "min_samples_split, \n",
    "min_samples_leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for decision tree\n",
    "class DecisionTree:\n",
    "     #defining initialization for decision tree\n",
    "    def __init__(self, criterion,m_dpth, min_smpls_leaf,sampl_featr=False):\n",
    "         #criterion for selecting  information_gain, gini_impurity\n",
    "        if criterion == 'entropy':\n",
    "            self.criterion = self._information_gain\n",
    "        elif criterion == 'gini':\n",
    "            self.criterion = self._gini_purification\n",
    "        else:\n",
    "            print('Criterion is information_gain_ratio or gini or entropy')\n",
    "        self._tree, self.sampl_featr, self.m_dpth, self.min_samples_leaf = None, sampl_featr, m_dpth, min_smpls_leaf\n",
    "        print(type(min_smpls_leaf))\n",
    "\n",
    "  # defining the fit method for the decision tree\n",
    "    def fit(self, X, y, sampl_wt=None):\n",
    "        if sampl_wt is None:\n",
    "            sampl_wt = np.ones(X.shape[0]) / X.shape[0]\n",
    "        else:\n",
    "            sampl_wt = np.array(sampl_wt) / np.sum(sampl_wt)\n",
    "\n",
    "        featr_names, X, y = X.columns.tolist(), np.array(X), np.array(y)\n",
    "        self._tree = self._build_tree(X, y, featr_names, depth=1, sample_weights=sampl_wt)\n",
    "        return self\n",
    "        \n",
    "   # defining entropy as static method \n",
    "    @staticmethod\n",
    "    def entropy(y, sampl_wt=None):\n",
    "        entrop, num = 0.0, y.shape[0]\n",
    "        labelCts={}\n",
    "        for i in range(num):\n",
    "            if y[i] not in labelCts.keys():\n",
    "                labelCts[y[i]]=0\n",
    "            labelCts[y[i]]+=sampl_wt[i]\n",
    "        for key in labelCts:\n",
    "            entrop-=float(labelCts[key])/float(np.sum(sampl_wt))*np.log2(float(labelCts[key])/float(np.sum(sampl_wt)))\n",
    "        return entrop\n",
    "\n",
    "  #defining information gain \n",
    "    def _information_gain(self, X, y, indx, sampl_wt=None):\n",
    "        in_gain, old_ct, uniq_val, new_ct = 0, self.entropy(y, sampl_wt), np.unique(X[:,indx]), 0.0\n",
    "        for value in uniq_val:\n",
    "            sub_X,sub_y, sub_sampl_wt=self._split_dataset(X, y, indx, value, sampl_wt)\n",
    "            prob=np.sum(sub_sampl_wt)/float(np.sum(sampl_wt))\n",
    "            new_ct+=prob*self.entropy(sub_y, sub_sampl_wt)\n",
    "        in_gain=old_ct-new_ct \n",
    "        return in_gain\n",
    "\n",
    "   # defining gini_impurity as static method with labelcounts\n",
    "    @staticmethod\n",
    "    def gini_impurity(y, sampl_wt=None):\n",
    "        gini_i, num = 1, y.shape[0]\n",
    "        labelCts={}\n",
    "        for i in range(num):\n",
    "            if y[i] not in labelCts.keys():\n",
    "                labelCts[y[i]]=0\n",
    "            labelCts[y[i]]+=sampl_wt[i]\n",
    "        for key in labelCts:\n",
    "            gini_i -= (float(labelCts[key])/float(np.sum(sampl_wt))) ** 2\n",
    "        return gini_i\n",
    "\n",
    "  #defining gini_purification\n",
    "    def _gini_purification(self, X, y, indx, sampl_wt=None):\n",
    "        n_impurity, old_ct, uniq_val, new_ct = 0, self.gini_impurity(y, sampl_wt), np.unique(X[:,indx]), 0.0\n",
    "        for value in uniq_val:\n",
    "            sub_X,sub_y, sub_sampl_wt=self._split_dataset(X, y, indx, value, sampl_wt)\n",
    "            new_ct+=np.sum(sub_sampl_wt)/float(np.sum(sampl_wt))*self.gini_impurity(sub_y, sub_sampl_wt)\n",
    "        n_impurity=old_ct-new_ct \n",
    "        return n_impurity\n",
    "\n",
    " #defining split dataset method\n",
    "    def _split_dataset(self, X, y, indx, val, sampl_wt=None):\n",
    "        rt, ftVec=[],X[:,indx]\n",
    "        X=X[:,[i for i in range(X.shape[1]) if i!=indx ]]\n",
    "        for i in range(len(ftVec)):\n",
    "            if ftVec[i]==val:\n",
    "                rt.append(i)\n",
    "        sub_X, sub_y, sub_sampl_wt = X[rt,:], y[rt], sampl_wt[rt]\n",
    "        return sub_X, sub_y, sub_sampl_wt\n",
    "\n",
    " #defining to choose best features\n",
    "    def _choose_best_feature(self, X, y, sampl_wt=None):\n",
    "        best_featr_idx, n_featr = 0, X.shape[1]\n",
    "        if self.sampl_featr:\n",
    "            max_featr=max(1, min(n_featr, int(np.round(np.sqrt(n_featr)))))\n",
    "            new_featr=np.random.choice(n_featr, max_featr, replace=False)\n",
    "            new_X=X[:, new_featr]\n",
    "        else:\n",
    "            new_X=X\n",
    "        n_new_featr, best_gain_ct=new_X.shape[1], 0.0\n",
    "        for i in range(n_new_featr):\n",
    "            info_gain_ct=self.criterion(new_X,y,i,sampl_wt)           \n",
    "            if info_gain_ct > best_gain_ct:\n",
    "                best_gain_ct, best_featr_idx=info_gain_ct, i          \n",
    "        return best_featr_idx\n",
    "        \n",
    " #defining majority vote as static method\n",
    "    @staticmethod\n",
    "    def majority_vote(y, sampl_wt=None):\n",
    "        if sampl_wt is None:\n",
    "            sampl_wt = np.ones(y.shape[0]) / y.shape[0]\n",
    "        majority_lbl = y[0]\n",
    "        dict_num={}\n",
    "        for i in range(y.shape[0]):\n",
    "            if int(y[i]) not in dict_num.keys():\n",
    "                dict_num[y[i]]=sampl_wt[i]\n",
    "            else:\n",
    "                dict_num[y[i]] += sampl_wt[i]\n",
    "        return max(dict_num, key=dict_num.get)\n",
    "\n",
    " #defining build tree for construction of decision tree\n",
    "    def _build_tree(self, X, y, feature_names, depth, sample_weights=None):\n",
    "        mytree = dict()\n",
    "        if len(feature_names)==0 or len(np.unique(y))==1 or int(depth) >= int(self.m_dpth) or int(len(X)) <= int(self.min_samples_leaf): \n",
    "            return self.majority_vote(y, sample_weights)\n",
    "        best_feature_idx=self._choose_best_feature(X, y, sample_weights)\n",
    "        best_feature_name = feature_names[best_feature_idx]\n",
    "        feature_names=feature_names[:]\n",
    "        feature_names.remove(best_feature_name)\n",
    "        mytree={best_feature_name:{}}\n",
    "        unique_vals=np.unique(X[:, best_feature_idx])\n",
    "        for value in unique_vals:\n",
    "            sub_X, sub_y, sub_sample_weights = self._split_dataset(X, y, best_feature_idx, value, sample_weights)\n",
    "            mytree[best_feature_name][value]=self._build_tree(sub_X, sub_y, feature_names, depth+1, sub_sample_weights)\n",
    "        return mytree\n",
    " #defining predict method for decision trees\n",
    "    def predict(self, X):\n",
    "         #defining classify method\n",
    "        def _classify(tree, x):\n",
    "            featr_name=list(tree.keys())[0]\n",
    "            s_dict, key=tree[featr_name], x.loc[featr_name]         \n",
    "            if key not in s_dict:\n",
    "                key=np.random.choice(list(s_dict.keys()))\n",
    "            valueOfKey=s_dict[key]\n",
    "            return isinstance(valueOfKey,dict) and _classify(valueOfKey,x) or valueOfKey\n",
    "        if len(X.shape)==1:\n",
    "            return _classify(self._tree,X)\n",
    "        else:\n",
    "            results=[]\n",
    "            for i in range(X.shape[0]):\n",
    "                results.append(_classify(self._tree, X.iloc[i, :]))\n",
    "            return np.array(results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Model Implementation  ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "012339ff-5353-268f-a97b-752aa4aa3602"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "Accuracy on train set: 0.2817574021012416\n",
      "Accuracy on test set: 0.25190839694656486\n"
     ]
    }
   ],
   "source": [
    " # implementing model of decision tree by calling decision tree class\n",
    "dt = DecisionTree(criterion='gini', m_dpth=4, min_smpls_leaf=4, sampl_featr=False)\n",
    "dt.fit(X, y)\n",
    "y_train_pred = dt.predict(X)\n",
    " # printing accuracy on train of the model generated\n",
    "print(\"Accuracy on train set: {}\".format(accuracy(y, y_train_pred)))\n",
    "# printing accuracy on test for the model generated\n",
    "print(\"Accuracy on test set: {}\".format(accuracy(y_test, dt.predict(X_test))))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    " #defining class random forest\n",
    "class RandomForest:\n",
    "    def __init__(self, tree_learner,n_estimator,state=2020):\n",
    "        np.random.seed(state)\n",
    "        self.n_estimator, self.weak_learner= n_estimator, tree_learner\n",
    "        self._estimators = [copy.deepcopy(self.weak_learner) for _ in range(self.n_estimator)]\n",
    "\n",
    "  #defining function to get bootstrap dataset\n",
    "    def _get_bootstrap_dataset(self, X, y, i):\n",
    "        X_arr, y_arr=np.array(X), np.array(y)\n",
    "        idx=np.random.choice(X_arr.shape[0], X_arr.shape[0], replace=True)\n",
    "        X_bootstrap= X.iloc[idx, :]\n",
    "        y_bootstrap = y.iloc[idx]\n",
    "        self._estimators[i].fit(X_bootstrap, y_bootstrap)\n",
    "\n",
    "  #defining fit for random forest classifier\n",
    "    def fit(self, X, y):\n",
    "        self.labels, part_btstrap, pool=list(set(y)), partial(self._get_bootstrap_dataset, X, y), ThreadPool(self.n_estimator)\n",
    "        pool.map(part_btstrap, list(range(self.n_estimator)))\n",
    "        pool.close() and pool.join()\n",
    "        return self\n",
    "\n",
    "  # defining predict for random forest classifier \n",
    "    def predict(self, X):\n",
    "        N = X.shape[0]\n",
    "        y_pred = np.zeros(N)\n",
    "        preds=np.array([estimator.predict(X) for estimator in self._estimators])\n",
    "        pred_proba=np.array([(preds == label).mean(axis=0) for label in self.labels]).T\n",
    "        y_pred=np.array(self.labels)[np.argmax(pred_proba, axis=1)]\n",
    "        return y_pred\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "035b62df-00f6-ab53-7082-227051ff974e"
   },
   "source": [
    "## Random forest Model Implementation  ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "Accuracy on train set: 0.6571155682903533\n",
      "Accuracy on test set: 0.6412213740458015\n"
     ]
    }
   ],
   "source": [
    "# implementing model for random forest\n",
    "base_learner = DecisionTree(criterion='gini', m_dpth=6, min_smpls_leaf=6, sampl_featr=True)\n",
    "r = RandomForest(tree_learner=base_learner, n_estimator=100, state=2020)\n",
    "r.fit(X, y)\n",
    "#predicting the training set\n",
    "y_train_pred = r.predict(X)\n",
    "# printing accuracy on train for the model generated\n",
    "print(\"Accuracy on train set: {}\".format(accuracy(y, y_train_pred)))\n",
    "# printing accuracy on test for the model generated\n",
    "print(\"Accuracy on test set: {}\".format(accuracy(y_test, r.predict(X_test))))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing AdaBoost as a second classifier(BOOSTING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    " # defining the boosting class\n",
    "class Adaboost:\n",
    "    # defining the initialization of method for boosting\n",
    "    def __init__(self,weak_learner,n_estimator,state=2020):\n",
    "        np.random.seed(state)\n",
    "        self.weak_learner, self.n_estimator = weak_learner, n_estimator\n",
    "        self._estimators = [copy.deepcopy(self.weak_learner) \n",
    "                            for _ in range(self.n_estimator)]\n",
    "        self._alphas = [1 \n",
    "                        for _ in range(n_estimator)]\n",
    "    # defining the fit method for boosting \n",
    "    def fit(self, X, y):\n",
    "        L=y.shape[0]\n",
    "        w=np.ones(L)/L\n",
    "        for i in range(self.n_estimator):\n",
    "            self._estimators[i].fit(X, y, sampl_wt=w)\n",
    "            pred=self._estimators[i].predict(X)\n",
    "            err=np.sum((pred!=y)*w)\n",
    "            alpha=0.5*np.log((1-err)/err)\n",
    "            self._alphas[i]=alpha\n",
    "            w*=np.exp(-alpha*np.array(y)*pred) + 1e-4\n",
    "            w/=2*np.sqrt(err*(1-err))\n",
    "        return self\n",
    "  #defining the predict method for boosting\n",
    "    def predict(self, X):\n",
    "        N = X.shape[0]\n",
    "        y_pred = np.zeros(N)\n",
    "        for i in range(self.n_estimator):\n",
    "            y_pred += self._alphas[i]*self._estimators[i].predict(X)   \n",
    "        return np.sign(y_pred)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost Model Implementation  ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "Accuracy on train set: 0.7793696275071633\n",
      "Accuracy on test set: 0.767175572519084\n"
     ]
    }
   ],
   "source": [
    " # implementing the model for adabboost boosting classifier\n",
    "base_learner = DecisionTree(criterion='entropy', m_dpth=2, min_smpls_leaf=1, sampl_featr=False)\n",
    "ada = Adaboost(weak_learner= base_learner, n_estimator=50, state=2020)\n",
    "ada.fit(X, y)\n",
    "#predicting the training set\n",
    "y_train_pred = ada.predict(X)\n",
    "# printing accuracy on train for the model generated\n",
    "print(\"Accuracy on train set: {}\".format(accuracy(y, y_train_pred)))\n",
    "# printing accuracy on test for the model generated\n",
    "print(\"Accuracy on test set: {}\".format(accuracy(y_test, ada.predict(X_test))))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance on Decision tree\n",
      "\n",
      "Accuracy on train set: 0.7793696275071633\n",
      "Accuracy on test set: 0.25190839694656486\n",
      "\n",
      "Model performance on Random Forest\n",
      "\n",
      "Accuracy on train set: 0.7793696275071633\n",
      "Accuracy on test set: 0.6412213740458015\n",
      "\n",
      "Model performance on Boosting(ADAboost)\n",
      "\n",
      "Accuracy on train set: 0.7793696275071633\n",
      "Accuracy on test set: 0.767175572519084\n"
     ]
    }
   ],
   "source": [
    "print('Model performance on Decision tree\\n')\n",
    " # printing accuracy on train of the model generated\n",
    "print(\"Accuracy on train set: {}\".format(accuracy(y, y_train_pred)))\n",
    "# printing accuracy on test for the model generated\n",
    "print(\"Accuracy on test set: {}\".format(accuracy(y_test, dt.predict(X_test))))\n",
    "print('\\nModel performance on Random Forest\\n')\n",
    "# printing accuracy on train for the model generated\n",
    "print(\"Accuracy on train set: {}\".format(accuracy(y, y_train_pred)))\n",
    "# printing accuracy on test for the model generated\n",
    "print(\"Accuracy on test set: {}\".format(accuracy(y_test, r.predict(X_test))))\n",
    "print('\\nModel performance on Boosting(ADAboost)\\n')\n",
    "# printing accuracy on train for the model generated\n",
    "print(\"Accuracy on train set: {}\".format(accuracy(y, y_train_pred)))\n",
    "# printing accuracy on test for the model generated\n",
    "print(\"Accuracy on test set: {}\".format(accuracy(y_test, ada.predict(X_test))))\n"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 3,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "886d5ead374c248a88e542b3362d4a2a699db30f125fd533335417ffeaaadd4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
